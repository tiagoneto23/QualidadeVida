{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relatório de Análise de Dados Socioeconômicos da PORDATA\n",
    "\n",
    "Este notebook apresenta a análise de dados socioeconômicos obtidos da PORDATA (Base de Dados de Portugal Contemporâneo), focando em indicadores relacionados à saúde e condições econômicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Este relatório apresenta a análise de dados socioeconômicos obtidos da PORDATA (Base de Dados de Portugal Contemporâneo), focando em indicadores relacionados à saúde e condições econômicas. O projeto foi desenvolvido como parte do trabalho prático da disciplina de Elementos de Inteligência Artificial e Ciência de Dados da Universidade da Beira Interior.\n",
    "\n",
    "A análise foi estruturada em seis fases principais: recolha de dados, limpeza e pré-processamento, exploração de dados, análise estatística, análise de relações entre variáveis e validação dos resultados. Cada fase empregou técnicas específicas implementadas através de scripts Python, permitindo uma abordagem sistemática e reproduzível para a extração de conhecimento a partir dos dados.\n",
    "\n",
    "Os datasets analisados incluem:\n",
    "- Ganho médio mensal\n",
    "- Esperança de vida\n",
    "- Despesas com saúde\n",
    "- Percepção de saúde\n",
    "- Taxa de mortalidade evitável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importação das bibliotecas necessárias para o projeto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Configurações para visualizações\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Definir diretório base\n",
    "DIRETORIO_BASE = \"C:/Users/fuguz/DATASETS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fase de Recolha de Dados\n",
    "\n",
    "### Técnicas Utilizadas\n",
    "\n",
    "A fase de recolha de dados foi implementada através do script `recolha_dados.py`, que utiliza técnicas de manipulação de arquivos e leitura de dados estruturados. O script emprega a biblioteca `pandas` para carregar os datasets em formato CSV e a biblioteca `os` para navegação no sistema de arquivos.\n",
    "\n",
    "Uma técnica importante implementada nesta fase foi a detecção automática de codificação de caracteres, permitindo lidar com diferentes formatos de codificação (UTF-16LE e Latin-1) frequentemente encontrados em datasets com caracteres especiais do português. Esta abordagem foi implementada através de um mecanismo de tratamento de exceções que tenta diferentes codificações quando a leitura inicial falha.\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "A implementação de um processo automatizado de recolha de dados foi essencial para garantir a reprodutibilidade da análise e facilitar futuras atualizações com novos dados. A função `recolha_dados()` centraliza este processo, permitindo que outros scripts importem os dados de forma consistente.\n",
    "\n",
    "A técnica de detecção automática de codificação foi necessária devido à presença de caracteres especiais nos nomes dos datasets e nos valores textuais, como \"ESPERANÇADEVIDA\" e \"PERCEÇAODESAUDE\". Sem este tratamento, os caracteres especiais poderiam ser interpretados incorretamente, comprometendo a integridade dos dados.\n",
    "\n",
    "### Resultados\n",
    "\n",
    "A função `recolha_dados()` conseguiu carregar com sucesso os cinco datasets da PORDATA, identificando automaticamente a codificação correta para cada arquivo. Os datasets foram armazenados em um dicionário Python, onde as chaves são os caminhos dos arquivos e os valores são os dataframes correspondentes.\n",
    "\n",
    "Esta fase estabeleceu a base para todas as análises subsequentes, garantindo que os dados brutos estivessem disponíveis em um formato adequado para processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implementação da função recolha_dados()\n",
    "def recolha_dados():\n",
    "    arquivos_desejados = [\n",
    "        'DESPESASAUDE.csv',\n",
    "        'ESPERANÇADEVIDA.csv',\n",
    "        'GANHOMEDIOMENSAL.csv',\n",
    "        'PERCEÇAODESAUDE.csv',\n",
    "        'TAXADEMORTALIDADEVITAVEL.csv'\n",
    "    ]\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    # Usamos a variável global DIRETORIO_BASE\n",
    "    for raiz, dirs, arquivos in os.walk(DIRETORIO_BASE):\n",
    "        for nome_ficheiro in arquivos:\n",
    "            if nome_ficheiro in arquivos_desejados:\n",
    "                caminho = os.path.join(raiz, nome_ficheiro)\n",
    "\n",
    "                try:\n",
    "                    # Tentativa de leitura com codificação 'utf-16le'\n",
    "                    df = pd.read_csv(caminho, encoding='utf-16le')\n",
    "                    datasets[caminho] = df\n",
    "                    print(f\"Arquivo carregado de {caminho}\")\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        # Tentativa com codificação 'latin1'\n",
    "                        df = pd.read_csv(caminho, encoding='latin1')\n",
    "                        datasets[caminho] = df\n",
    "                        print(f\"Arquivo carregado de {caminho} com codificação 'latin1'\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erro ao ler {caminho} com codificação 'latin1': {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao ler {caminho}: {e}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# Executar a função para carregar os datasets\n",
    "datasets = recolha_dados()\n",
    "\n",
    "# Exibir informações básicas sobre os datasets carregados\n",
    "for caminho, df in datasets.items():\n",
    "    nome = os.path.basename(caminho)\n",
    "    print(f\"\\nDataset: {nome}\")\n",
    "    print(f\"Dimensões: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
    "    print(f\"Colunas: {', '.join(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fase de Limpeza e Pré-processamento\n",
    "\n",
    "### Técnicas Utilizadas\n",
    "\n",
    "A limpeza e pré-processamento dos dados foram implementados no script `limpeza_dados.py`, que utiliza técnicas de transformação e filtragem de dados. A função principal `limpar_preprocessar()` aplica uma série de operações para preparar os dados para análise:\n",
    "\n",
    "1. Padronização de nomes de colunas para facilitar o acesso\n",
    "2. Conversão de tipos de dados (anos para inteiros, valores para numéricos)\n",
    "3. Tratamento de valores ausentes (remoção de linhas sem valores e preenchimento de campos categóricos vazios)\n",
    "4. Adição de metadados específicos para cada dataset (indicador e unidade de medida)\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "A fase de limpeza e pré-processamento é crucial para garantir a qualidade e consistência dos dados antes da análise. A padronização dos nomes de colunas foi necessária para unificar a estrutura dos diferentes datasets, facilitando a integração posterior.\n",
    "\n",
    "A conversão de tipos de dados foi implementada para garantir que operações matemáticas e estatísticas pudessem ser aplicadas corretamente. Por exemplo, a conversão da coluna \"Ano\" para inteiro permite ordenação cronológica adequada, enquanto a conversão da coluna \"Valor\" para numérico possibilita cálculos estatísticos.\n",
    "\n",
    "O tratamento de valores ausentes foi essencial para evitar distorções nas análises estatísticas. A decisão de remover linhas sem valores na coluna principal (\"Valor\") foi tomada para preservar a integridade das análises, enquanto o preenchimento de campos categóricos vazios com strings vazias foi adotado para manter a consistência estrutural.\n",
    "\n",
    "### Resultados\n",
    "\n",
    "O processo de limpeza resultou em datasets estruturalmente consistentes e prontos para análise. Cada dataset foi salvo em formato CSV com o sufixo \"_limpo\", preservando os dados originais e criando versões processadas para as fases subsequentes.\n",
    "\n",
    "A função `limpar_preprocessar()` gerou um relatório detalhado do processo, incluindo o número de linhas removidas por valores ausentes e as transformações aplicadas a cada dataset. Este relatório foi salvo no arquivo \"resumo_limpeza.txt\", permitindo a verificação da qualidade do processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implementação da função limpar_preprocessar()\n",
    "def limpar_preprocessar(df, nome_dataset):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Limpeza e pré-processamento do dataset: {nome_dataset}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Cópia do dataframe original para não modificá-lo\n",
    "    df_limpo = df.copy()\n",
    "\n",
    "    # 1. Renomear colunas para facilitar o acesso\n",
    "    colunas_originais = df_limpo.columns.tolist()\n",
    "    colunas_novas = [\n",
    "        'Ano', 'Pais', 'Regiao', 'Filtro1', 'Filtro2', 'Filtro3', 'Escala', 'Simbolo', 'Valor'\n",
    "    ]\n",
    "\n",
    "    # Verificar se o número de colunas corresponde\n",
    "    if len(colunas_originais) == len(colunas_novas):\n",
    "        df_limpo.columns = colunas_novas\n",
    "        print(f\"Colunas renomeadas: {colunas_originais} -> {colunas_novas}\")\n",
    "    else:\n",
    "        print(f\"Erro ao renomear colunas: número de colunas não corresponde ({len(colunas_originais)} vs {len(colunas_novas)})\")\n",
    "\n",
    "    # 2. Converter a coluna Ano para inteiro (quando possível)\n",
    "    try:\n",
    "        # Primeiro, remover valores não numéricos\n",
    "        df_limpo['Ano'] = pd.to_numeric(df_limpo['Ano'], errors='coerce')\n",
    "        # Converter para inteiro, mantendo NaN onde necessário\n",
    "        df_limpo['Ano'] = df_limpo['Ano'].astype('Int64')\n",
    "        print(\"Coluna 'Ano' convertida para inteiro\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao converter coluna 'Ano' para inteiro: {e}\")\n",
    "\n",
    "    # 3. Tratar valores ausentes\n",
    "    df_limpo = df_limpo.dropna(subset=['Valor'])\n",
    "    print(f\"Linhas removidas por valor ausente: {len(df) - len(df_limpo)}\")\n",
    "\n",
    "    for col in ['Pais', 'Regiao', 'Filtro1', 'Filtro2', 'Filtro3', 'Escala', 'Simbolo']:\n",
    "        df_limpo[col] = df_limpo[col].fillna('')\n",
    "\n",
    "    # 4. Converter a coluna Valor para numérico\n",
    "    df_limpo['Valor'] = pd.to_numeric(df_limpo['Valor'], errors='coerce')\n",
    "    print(\"Coluna 'Valor' convertida para numérico\")\n",
    "\n",
    "    # 5. Adicionar metadados específicos para cada dataset\n",
    "    if nome_dataset == 'GANHOMEDIOMENSAL':\n",
    "        df_limpo['Indicador'] = 'Ganho Médio Mensal'\n",
    "        df_limpo['Unidade'] = 'euros'\n",
    "    elif nome_dataset == 'ESPERANÇADEVIDA':\n",
    "        df_limpo['Indicador'] = 'Esperança de Vida'\n",
    "        df_limpo['Unidade'] = 'anos'\n",
    "    elif nome_dataset == 'DESPESASAUDE':\n",
    "        df_limpo['Indicador'] = 'Despesa em Saúde'\n",
    "        df_limpo['Unidade'] = df_limpo['Escala'].str.strip()\n",
    "    elif nome_dataset == 'PERCEÇAODESAUDE':\n",
    "        df_limpo['Indicador'] = 'Percepção de Saúde'\n",
    "        df_limpo['Unidade'] = '%'\n",
    "    elif nome_dataset == 'TAXADEMORTALIDADEVITAVEL':\n",
    "        df_limpo['Indicador'] = 'Taxa de Mortalidade Evitável'\n",
    "        df_limpo['Unidade'] = 'por 100 mil habitantes'\n",
    "\n",
    "    # 6. Retornar dataset limpo\n",
    "    return df_limpo\n",
    "\n",
    "# Processar os datasets\n",
    "datasets_limpos = {}\n",
    "for caminho, df in datasets.items():\n",
    "    nome_dataset = os.path.basename(caminho)\n",
    "    print(f\"\\nProcessando dataset: {nome_dataset}\")\n",
    "    \n",
    "    # Limpar e processar os dados\n",
    "    df_limpo = limpar_preprocessar(df, nome_dataset)\n",
    "    datasets_limpos[nome_dataset] = df_limpo\n",
    "    \n",
    "    # Salvar dataset limpo\n",
    "    caminho_limpo = os.path.join(DIRETORIO_BASE, f\"{nome_dataset}_limpo.csv\")\n",
    "    df_limpo.to_csv(caminho_limpo, index=False)\n",
    "    print(f\"Dataset limpo salvo em: {caminho_limpo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fase de Exploração de Dados\n",
    "\n",
    "### Técnicas Utilizadas\n",
    "\n",
    "A exploração inicial dos dados foi implementada no script `explorar_datasets.py`, que utiliza técnicas de análise descritiva e visualização para compreender a estrutura e características dos datasets. A função principal `analisar_estrutura()` aplica as seguintes técnicas:\n",
    "\n",
    "1. Análise de dimensionalidade (número de linhas e colunas)\n",
    "2. Exame das primeiras linhas para compreensão do conteúdo\n",
    "3. Análise de tipos de dados e valores nulos\n",
    "4. Estatísticas descritivas para colunas numéricas\n",
    "5. Análise de valores únicos para colunas categóricas\n",
    "\n",
    "O script utiliza a biblioteca `tabulate` para formatação tabular dos resultados, facilitando a leitura e interpretação das informações.\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "A fase de exploração é fundamental para compreender a natureza dos dados antes de aplicar técnicas analíticas mais avançadas. A análise de dimensionalidade permite avaliar o volume de dados disponível para cada indicador, enquanto o exame das primeiras linhas proporciona uma visão geral do conteúdo.\n",
    "\n",
    "A análise de tipos de dados e valores nulos é essencial para identificar potenciais problemas que poderiam afetar as análises subsequentes. As estatísticas descritivas fornecem uma visão inicial da distribuição dos dados numéricos, enquanto a análise de valores únicos para colunas categóricas ajuda a compreender a diversidade de categorias presentes.\n",
    "\n",
    "### Resultados\n",
    "\n",
    "A exploração de dados gerou um resumo abrangente para cada dataset, incluindo dimensões, estrutura de colunas e período temporal coberto. Este resumo foi consolidado no arquivo \"resumo_datasets.txt\", que serve como referência para compreender o escopo e as características dos dados.\n",
    "\n",
    "A análise revelou que os datasets cobrem diferentes períodos temporais, com o dataset de esperança de vida apresentando a maior amplitude (1960-2024) e o dataset de percepção de saúde a menor (2005-2023). Também foi identificada a presença de valores nulos em algumas colunas, confirmando a necessidade das operações de limpeza realizadas na fase anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implementação da função analisar_estrutura()\n",
    "def analisar_estrutura(df, nome):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Análise do dataset: {nome}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Informações básicas\n",
    "    print(f\"\\nDimensões: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
    "\n",
    "    # Primeiras linhas\n",
    "    print(\"\\nPrimeiras 5 linhas:\")\n",
    "    display(df.head())\n",
    "\n",
    "    # Informações sobre as colunas\n",
    "    print(\"\\nInformações sobre as colunas:\")\n",
    "    info = pd.DataFrame({\n",
    "        'Tipo': df.dtypes,\n",
    "        'Valores não nulos': df.count(),\n",
    "        'Valores nulos': df.isnull().sum(),\n",
    "        '% Valores nulos': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        'Valores únicos': df.nunique()\n",
    "    })\n",
    "    display(info)\n",
    "\n",
    "    # Estatísticas descritivas para colunas numéricas\n",
    "    colunas_numericas = df.select_dtypes(include=['number']).columns\n",
    "    if len(colunas_numericas) > 0:\n",
    "        print(\"\\nEstatísticas descritivas para colunas numéricas:\")\n",
    "        desc = df[colunas_numericas].describe().T\n",
    "        display(desc)\n",
    "\n",
    "    # Valores únicos para colunas categóricas (limitado a 10)\n",
    "    colunas_categoricas = df.select_dtypes(exclude=['number']).columns\n",
    "    if len(colunas_categoricas) > 0:\n",
    "        print(\"\\nValores únicos para colunas categóricas (limitado a 10):\")\n",
    "        for col in colunas_categoricas:\n",
    "            valores_unicos = df[col].unique()\n",
    "            print(f\"\\n{col}: {len(valores_unicos)} valores únicos\")\n",
    "            if len(valores_unicos) <= 10:\n",
    "                print(valores_unicos)\n",
    "            else:\n",
    "                print(valores_unicos[:10], \"... (mais valores)\")\n",
    "\n",
    "    return info\n",
    "\n",
    "# Explorar os datasets limpos\n",
    "resumo_exploracao = {}\n",
    "for nome, df in datasets_limpos.items():\n",
    "    print(f\"\\nExplorando dataset: {nome}\")\n",
    "    info = analisar_estrutura(df, nome)\n",
    "    resumo_exploracao[nome] = {\n",
    "        'dimensoes': df.shape,\n",
    "        'periodo': (df['Ano'].min(), df['Ano'].max()) if 'Ano' in df.columns else None,\n",
    "        'paises': df['Pais'].unique() if 'Pais' in df.columns else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fase de Análise Estatística\n",
    "\n",
    "### Técnicas Utilizadas\n",
    "\n",
    "A análise estatística foi implementada no script `analise_estatistica.py`, que aplica técnicas estatísticas descritivas e inferenciais para extrair insights dos dados. As principais funções incluem:\n",
    "\n",
    "1. `analise_estatistica_descritiva()`: Calcula estatísticas descritivas (média, mediana, desvio padrão, etc.) e realiza testes de normalidade (Shapiro-Wilk)\n",
    "2. `criar_visualizacoes_basicas()`: Gera histogramas e gráficos de evolução temporal para cada indicador\n",
    "3. `analisar_correlacoes()`: Calcula e visualiza a matriz de correlação entre os diferentes indicadores\n",
    "\n",
    "O script utiliza as bibliotecas `matplotlib` e `seaborn` para visualizações e `scipy.stats` para testes estatísticos.\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "A análise estatística é essencial para quantificar padrões e relações nos dados. As estatísticas descritivas fornecem uma visão geral da distribuição dos valores para cada indicador, enquanto os testes de normalidade ajudam a determinar se os dados seguem uma distribuição normal, o que influencia a escolha de métodos estatísticos subsequentes.\n",
    "\n",
    "As visualizações básicas (histogramas e gráficos de evolução temporal) foram implementadas para facilitar a interpretação dos resultados estatísticos, permitindo identificar visualmente tendências e padrões que poderiam não ser evidentes apenas com números.\n",
    "\n",
    "A análise de correlação foi aplicada para identificar relações lineares entre os diferentes indicadores, fornecendo uma base para investigações mais aprofundadas nas fases subsequentes.\n",
    "\n",
    "### Resultados\n",
    "\n",
    "A análise estatística revelou características importantes dos dados, como a distribuição não-normal da maioria dos indicadores (confirmada pelos testes de Shapiro-Wilk) e a presença de tendências temporais consistentes.\n",
    "\n",
    "Os histogramas mostraram distribuições distintas para cada indicador, com o ganho médio mensal apresentando uma distribuição mais assimétrica (positivamente enviesada) e a esperança de vida uma distribuição mais próxima da normal.\n",
    "\n",
    "A matriz de correlação identificou relações significativas entre os indicadores, destacando-se:\n",
    "- Forte correlação positiva (0.983) entre ganho médio mensal e despesas de saúde\n",
    "- Correlação positiva (0.869) entre ganho médio mensal e percepção de saúde\n",
    "- Correlação negativa (-0.798) entre esperança de vida e taxa de mortalidade evitável\n",
    "\n",
    "Estas correlações forneceram direções para a análise mais aprofundada na fase seguinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implementação da função analise_estatistica_descritiva()\n",
    "def analise_estatistica_descritiva(datasets):\n",
    "    resultados = {}\n",
    "    for nome, df in datasets.items():\n",
    "        print(f\"\\n{'='*50}\\nAnálise Estatística: {nome}\\n{'='*50}\")\n",
    "        try:\n",
    "            estatisticas = df['Valor'].describe()\n",
    "            print(\"Estatísticas descritivas:\")\n",
    "            display(estatisticas)\n",
    "            \n",
    "            # Teste de normalidade (Shapiro-Wilk)\n",
    "            amostra = df['Valor'].dropna().sample(min(5000, len(df['Valor'].dropna())))\n",
    "            shapiro = stats.shapiro(amostra)\n",
    "            print(f\"\\nShapiro-Wilk: estatística = {shapiro.statistic:.4f}, p-valor = {shapiro.pvalue:.4f}\")\n",
    "            if shapiro.pvalue < 0.05:\n",
    "                print(\"Os dados não seguem uma distribuição normal (p < 0.05)\")\n",
    "            else:\n",
    "                print(\"Os dados seguem uma distribuição normal (p >= 0.05)\")\n",
    "\n",
    "            # Evolução temporal\n",
    "            evolucao = df.groupby('Ano')['Valor'].mean()\n",
    "            taxa_crescimento = evolucao.pct_change() * 100\n",
    "            taxa_media = taxa_crescimento.mean()\n",
    "            print(f\"\\nTaxa de crescimento média anual: {taxa_media:.2f}%\")\n",
    "\n",
    "            resultados[nome] = {\n",
    "                'estatisticas': estatisticas,\n",
    "                'normalidade': shapiro,\n",
    "                'evolucao_temporal': evolucao,\n",
    "                'taxa_crescimento_media': taxa_media\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na análise de {nome}: {e}\")\n",
    "    return resultados\n",
    "\n",
    "# Implementação da função criar_visualizacoes_basicas()\n",
    "def criar_visualizacoes_basicas(datasets):\n",
    "    for nome, df in datasets.items():\n",
    "        try:\n",
    "            # Histograma\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.histplot(df['Valor'].dropna(), kde=True)\n",
    "            plt.title(f\"Distribuição - {nome}\", fontsize=14)\n",
    "            plt.xlabel(\"Valor\", fontsize=12)\n",
    "            plt.ylabel(\"Frequência\", fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Evolução temporal\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            df.groupby('Ano')['Valor'].mean().plot(marker='o')\n",
    "            plt.title(f\"Evolução Temporal - {nome}\", fontsize=14)\n",
    "            plt.xlabel(\"Ano\", fontsize=12)\n",
    "            plt.ylabel(\"Valor Médio\", fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Erro nas visualizações de {nome}: {e}\")\n",
    "\n",
    "# Implementação da função analisar_correlacoes()\n",
    "def analisar_correlacoes(datasets):\n",
    "    # Identificar anos e países comuns\n",
    "    anos = [set(df['Ano'].unique()) for df in datasets.values() if 'Ano' in df.columns]\n",
    "    paises = [set(df['Pais'].dropna().unique()) for df in datasets.values() if 'Pais' in df.columns]\n",
    "    anos_comuns = set.intersection(*anos) if anos else set()\n",
    "    paises_comuns = set.intersection(*paises) if paises else set()\n",
    "\n",
    "    if not anos_comuns or not paises_comuns:\n",
    "        print(\"Anos ou países comuns insuficientes para análise de correlação.\")\n",
    "        return None, None\n",
    "\n",
    "    # Criar dataframe integrado\n",
    "    df_geral = pd.DataFrame()\n",
    "    for nome, df in datasets.items():\n",
    "        if 'Pais' in df.columns:\n",
    "            df_filtrado = df[df['Ano'].isin(anos_comuns) & df['Pais'].isin(paises_comuns)]\n",
    "            df_medio = df_filtrado.groupby(['Pais', 'Ano'])['Valor'].mean().reset_index()\n",
    "            df_medio = df_medio.rename(columns={'Valor': nome})\n",
    "            df_geral = df_medio if df_geral.empty else pd.merge(df_geral, df_medio, on=['Pais', 'Ano'])\n",
    "\n",
    "    # Calcular matriz de correlação\n",
    "    corr = df_geral.drop(['Pais', 'Ano'], axis=1).corr()\n",
    "    print(\"\\nMatriz de Correlação:\")\n",
    "    display(corr)\n",
    "\n",
    "    # Visualizar matriz de correlação\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(\"Matriz de Correlação entre Indicadores\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_geral, corr\n",
    "\n",
    "# Executar análise estatística\n",
    "resultados_estatisticos = analise_estatistica_descritiva(datasets_limpos)\n",
    "criar_visualizacoes_basicas(datasets_limpos)\n",
    "df_integrado, matriz_correlacao = analisar_correlacoes(datasets_limpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fase de Análise de Relações\n",
    "\n",
    "### Técnicas Utilizadas\n",
    "\n",
    "A análise de relações entre variáveis foi implementada no script `analise_relacoes.py`, que aplica técnicas mais avançadas para investigar as conexões identificadas na fase anterior. As principais funções incluem:\n",
    "\n",
    "1. `criar_dataframe_integrado()`: Integra dados de diferentes datasets para análise conjunta\n",
    "2. `analisar_correlacoes()`: Realiza análise detalhada das correlações mais significativas\n",
    "3. `analisar_regressoes()`: Aplica modelos de regressão linear para quantificar relações\n",
    "4. `analisar_pca()`: Implementa Análise de Componentes Principais para redução de dimensionalidade\n",
    "5. `analisar_clusters()`: Aplica algoritmos de clustering para identificar grupos de países com características similares\n",
    "6. `analisar_tendencias_temporais()`: Examina padrões de evolução ao longo do tempo\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "A análise de relações é fundamental para extrair conhecimento mais profundo dos dados, indo além das estatísticas descritivas. A integração de dados de diferentes datasets foi necessária para permitir análises conjuntas, superando a limitação de ter os indicadores em arquivos separados.\n",
    "\n",
    "A análise de regressão foi aplicada para quantificar as relações identificadas na matriz de correlação, permitindo estimar o impacto de um indicador sobre outro. Esta técnica é particularmente útil para relações com forte correlação linear.\n",
    "\n",
    "A Análise de Componentes Principais (PCA) foi implementada para lidar com a multicolinearidade entre variáveis e identificar dimensões latentes que explicam a variabilidade dos dados. Esta técnica é valiosa para reduzir a complexidade dos dados multidimensionais.\n",
    "\n",
    "A análise de clusters foi aplicada para identificar grupos de países com perfis socioeconômicos similares, permitindo uma visão mais estruturada da diversidade entre os países europeus.\n",
    "\n",
    "### Resultados\n",
    "\n",
    "A análise de relações produziu insights significativos sobre as conexões entre os indicadores socioeconômicos:\n",
    "\n",
    "A regressão linear entre esperança de vida e taxa de mortalidade evitável resultou na equação: Taxa de mortalidade = -23.32 × Esperança de vida + 695.50, com R² = 0.637, indicando que 63.7% da variação na taxa de mortalidade evitável pode ser explicada pela esperança de vida.\n",
    "\n",
    "A Análise de Componentes Principais identificou que dois componentes principais explicam aproximadamente 85% da variância total dos dados, sugerindo que a dimensionalidade efetiva dos dados é menor que o número de indicadores originais.\n",
    "\n",
    "A análise de clusters identificou três grupos distintos de países europeus com base nos indicadores analisados:\n",
    "- Cluster 1: Países nórdicos e Europa Ocidental, caracterizados por alto ganho médio, alta esperança de vida e baixa mortalidade evitável\n",
    "- Cluster 2: Países do Sul e Leste Europeu, incluindo Portugal, com valores intermediários\n",
    "- Cluster 3: Países do Leste Europeu com indicadores socioeconômicos mais baixos\n",
    "\n",
    "A análise de tendências temporais revelou melhorias consistentes em todos os indicadores para Portugal ao longo do período analisado, com taxas de crescimento anual médias de:\n",
    "- Ganho médio mensal: +2.78%\n",
    "- Esperança de vida: +0.22%\n",
    "- Despesas de saúde: +3.44%\n",
    "- Percepção de saúde: +0.09%\n",
    "- Taxa de mortalidade evitável: -0.57%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implementação da função analisar_regressoes()\n",
    "def analisar_regressoes(df_integrado):\n",
    "    print(\"\\n2. Análise de Regressão\")\n",
    "\n",
    "    # Colunas numéricas (excluindo Ano)\n",
    "    colunas_numericas = df_integrado.select_dtypes(include=['number']).columns.tolist()\n",
    "    if 'Ano' in colunas_numericas:\n",
    "        colunas_numericas.remove('Ano')\n",
    "\n",
    "    # Calcular matriz de correlação para identificar pares para regressão\n",
    "    matriz_correlacao = df_integrado[colunas_numericas].corr()\n",
    "\n",
    "    # Identificar correlações mais fortes (positivas e negativas)\n",
    "    correlacoes = []\n",
    "    for i in range(len(matriz_correlacao.columns)):\n",
    "        for j in range(i + 1, len(matriz_correlacao.columns)):\n",
    "            col1 = matriz_correlacao.columns[i]\n",
    "            col2 = matriz_correlacao.columns[j]\n",
    "            corr = matriz_correlacao.iloc[i, j]\n",
    "            correlacoes.append((col1, col2, corr))\n",
    "\n",
    "    # Ordenar por valor absoluto de correlação\n",
    "    correlacoes_ordenadas = sorted(correlacoes, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "    # Analisar regressão para os 3 pares com correlações mais fortes\n",
    "    resultados_regressao = []\n",
    "\n",
    "    for col1, col2, corr in correlacoes_ordenadas[:3]:\n",
    "        print(f\"\\nAnalisando regressão: {col1} vs {col2}\")\n",
    "\n",
    "        # Remover linhas com valores nulos\n",
    "        df_temp = df_integrado[[col1, col2]].dropna()\n",
    "\n",
    "        # Verificar se há dados suficientes\n",
    "        if len(df_temp) < 10:\n",
    "            print(f\"Dados insuficientes para análise de regressão entre {col1} e {col2}\")\n",
    "            continue\n",
    "\n",
    "        # Realizar regressão linear\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(df_temp[col1], df_temp[col2])\n",
    "\n",
    "        print(f\"Equação da reta: {col2} = {slope:.4f} * {col1} + {intercept:.4f}\")\n",
    "        print(f\"R²: {r_value ** 2:.4f}\")\n",
    "        print(f\"p-valor: {p_value:.4f}\")\n",
    "\n",
    "        # Armazenar resultados\n",
    "        resultados_regressao.append({\n",
    "            'var_x': col1,\n",
    "            'var_y': col2,\n",
    "            'slope': slope,\n",
    "            'intercept': intercept,\n",
    "            'r_squared': r_value ** 2,\n",
    "            'p_value': p_value,\n",
    "            'std_err': std_err\n",
    "        })\n",
    "\n",
    "        # Criar gráfico de regressão\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.regplot(x=col1, y=col2, data=df_temp, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})\n",
    "        plt.title(f'Regressão Linear: {col1} vs {col2} (R² = {r_value ** 2:.4f})', fontsize=16)\n",
    "        plt.xlabel(col1, fontsize=14)\n",
    "        plt.ylabel(col2, fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return resultados_regressao\n",
    "\n",
    "# Implementação da função analisar_pca()\n",
    "def analisar_pca(df_integrado):\n",
    "    print(\"\\n3. Análise de Componentes Principais (PCA)\")\n",
    "\n",
    "    # Colunas numéricas (excluindo Ano)\n",
    "    colunas_numericas = df_integrado.select_dtypes(include=['number']).columns.tolist()\n",
    "    if 'Ano' in colunas_numericas:\n",
    "        colunas_numericas.remove('Ano')\n",
    "\n",
    "    # Verificar se há dados suficientes\n",
    "    if len(colunas_numericas) < 2:\n",
    "        print(\"Dados insuficientes para análise PCA (menos de 2 variáveis numéricas)\")\n",
    "        return None\n",
    "\n",
    "    # Remover linhas com valores nulos\n",
    "    df_pca = df_integrado[colunas_numericas].dropna()\n",
    "\n",
    "    if len(df_pca) < 10:\n",
    "        print(\"Dados insuficientes para análise PCA (menos de 10 observações completas)\")\n",
    "        return None\n",
    "\n",
    "    # Padronizar os dados\n",
    "    scaler = StandardScaler()\n",
    "    dados_padronizados = scaler.fit_transform(df_pca)\n",
    "\n",
    "    # Aplicar PCA\n",
    "    pca = PCA()\n",
    "    componentes_principais = pca.fit_transform(dados_padronizados)\n",
    "\n",
    "    # Variância explicada\n",
    "    variancia_explicada = pca.explained_variance_ratio_\n",
    "    variancia_acumulada = np.cumsum(variancia_explicada)\n",
    "\n",
    "    print(\"\\nVariância explicada por componente:\")\n",
    "    for i, var in enumerate(variancia_explicada):\n",
    "        print(f\"PC{i + 1}: {var:.4f} ({variancia_acumulada[i]:.4f} acumulada)\")\n",
    "\n",
    "    # Criar gráfico de variância explicada\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(1, len(variancia_explicada) + 1), variancia_explicada, alpha=0.7, label='Individual')\n",
    "    plt.step(range(1, len(variancia_acumulada) + 1), variancia_acumulada, where='mid', label='Acumulada')\n",
    "    plt.axhline(y=0.8, color='r', linestyle='--', alpha=0.5, label='Limite 80%')\n",
    "    plt.title('Variância Explicada por Componente Principal', fontsize=16)\n",
    "    plt.xlabel('Componente Principal', fontsize=14)\n",
    "    plt.ylabel('Proporção de Variância Explicada', fontsize=14)\n",
    "    plt.xticks(range(1, len(variancia_explicada) + 1))\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pca, componentes_principais\n",
    "\n",
    "# Implementação da função analisar_clusters()\n",
    "def analisar_clusters(df_integrado):\n",
    "    print(\"\\n4. Análise de Clusters\")\n",
    "    \n",
    "    # Colunas numéricas (excluindo Ano)\n",
    "    colunas_numericas = df_integrado.select_dtypes(include=['number']).columns.tolist()\n",
    "    if 'Ano' in colunas_numericas:\n",
    "        colunas_numericas.remove('Ano')\n",
    "    \n",
    "    # Verificar se há dados suficientes\n",
    "    if len(colunas_numericas) < 2:\n",
    "        print(\"Dados insuficientes para análise de clusters (menos de 2 variáveis numéricas)\")\n",
    "        return None\n",
    "    \n",
    "    # Preparar dados para clustering\n",
    "    # Usar a média por país para agrupar países similares\n",
    "    df_cluster = df_integrado.groupby('Pais')[colunas_numericas].mean().reset_index()\n",
    "    \n",
    "    if len(df_cluster) < 5:\n",
    "        print(\"Dados insuficientes para análise de clusters (menos de 5 países)\")\n",
    "        return None\n",
    "    \n",
    "    # Padronizar os dados\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df_cluster[colunas_numericas])\n",
    "    \n",
    "    # Determinar número ideal de clusters usando o método do cotovelo\n",
    "    inertias = []\n",
    "    silhouettes = []\n",
    "    max_clusters = min(10, len(df_cluster) - 1)\n",
    "    range_clusters = range(2, max_clusters + 1)\n",
    "    \n",
    "    for k in range_clusters:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        if k > 1:\n",
    "            silhouette = silhouette_score(X, kmeans.labels_)\n",
    "            silhouettes.append(silhouette)\n",
    "            print(f\"k={k}, silhouette={silhouette:.4f}\")\n",
    "    \n",
    "    # Plotar método do cotovelo\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range_clusters, inertias, 'o-')\n",
    "    plt.title('Método do Cotovelo', fontsize=14)\n",
    "    plt.xlabel('Número de Clusters', fontsize=12)\n",
    "    plt.ylabel('Inércia', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(3, max_clusters + 1), silhouettes, 'o-')\n",
    "    plt.title('Coeficiente de Silhueta', fontsize=14)\n",
    "    plt.xlabel('Número de Clusters', fontsize=12)\n",
    "    plt.ylabel('Silhueta', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Escolher número de clusters com base no coeficiente de silhueta\n",
    "    optimal_k = silhouettes.index(max(silhouettes)) + 3  # +3 porque começamos de k=3\n",
    "    print(f\"\\nNúmero ótimo de clusters baseado na silhueta: {optimal_k}\")\n",
    "    \n",
    "    # Aplicar K-means com número ótimo de clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    df_cluster['Cluster'] = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Analisar características dos clusters\n",
    "    print(\"\\nCaracterísticas dos clusters:\")\n",
    "    cluster_stats = df_cluster.groupby('Cluster')[colunas_numericas].mean()\n",
    "    display(cluster_stats)\n",
    "    \n",
    "    # Visualizar clusters (usando PCA para redução de dimensionalidade)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_cluster['Cluster'], cmap='viridis', s=100, alpha=0.8)\n",
    "    \n",
    "    # Adicionar rótulos dos países\n",
    "    for i, txt in enumerate(df_cluster['Pais']):\n",
    "        plt.annotate(txt, (X_pca[i, 0], X_pca[i, 1]), fontsize=10, alpha=0.7)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title('Visualização dos Clusters de Países (PCA)', fontsize=16)\n",
    "    plt.xlabel(f'Componente Principal 1 ({pca.explained_variance_ratio_[0]:.2%})', fontsize=14)\n",
    "    plt.ylabel(f'Componente Principal 2 ({pca.explained_variance_ratio_[1]:.2%})', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_cluster\n",
    "\n",
    "# Executar análise de relações\n",
    "if df_integrado is not None:\n",
    "    resultados_regressao = analisar_regressoes(df_integrado)\n",
    "    pca_results = analisar_pca(df_integrado)\n",
    "    cluster_results = analisar_clusters(df_integrado)\n",
    "else:\n",
    "    print(\"Não foi possível realizar a análise de relações devido à falta de um dataframe integrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fase de Validação dos Resultados\n",
    "\n",
    "### Técnicas Utilizadas\n",
    "\n",
    "A validação dos resultados foi implementada no script `validar_resultados.py`, que aplica técnicas para verificar a robustez e confiabilidade das análises realizadas. As principais funções incluem:\n",
    "\n",
    "1. `verificar_arquivos_resultados()`: Verifica a existência de todos os arquivos de resultados esperados\n",
    "2. `validar_consistencia_dados()`: Avalia a consistência dos dados limpos\n",
    "3. `verificar_robustez_correlacoes()`: Testa a sensibilidade das correlações à remoção de observações\n",
    "4. `validar_analises_regressao()`: Verifica os pressupostos dos modelos de regressão\n",
    "5. `verificar_tendencias_temporais()`: Avalia a consistência das tendências identificadas\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "A validação é uma fase crítica para garantir a confiabilidade dos resultados e identificar potenciais limitações. A verificação da existência de arquivos garante que todas as etapas anteriores foram concluídas com sucesso, enquanto a validação da consistência dos dados confirma que a limpeza foi eficaz.\n",
    "\n",
    "A verificação da robustez das correlações é essencial para determinar se as relações identificadas são estáveis ou se são influenciadas por outliers ou pequenos conjuntos de observações. Esta técnica envolve a remoção sistemática de observações e a reavaliação das correlações.\n",
    "\n",
    "A validação das análises de regressão verifica se os pressupostos do modelo linear (linearidade, normalidade dos resíduos, homocedasticidade) são atendidos, o que é fundamental para a validade das inferências baseadas nesses modelos.\n",
    "\n",
    "### Resultados\n",
    "\n",
    "A validação dos resultados identificou algumas limitações importantes nas análises:\n",
    "\n",
    "A verificação de robustez das correlações revelou que algumas correlações são sensíveis à remoção de observações específicas, particularmente a correlação entre percepção de saúde e taxa de mortalidade evitável, que variou significativamente quando certos países foram removidos da análise.\n",
    "\n",
    "A validação das análises de regressão identificou violações moderadas do pressuposto de normalidade dos resíduos em alguns modelos, sugerindo cautela na interpretação dos p-valores e intervalos de confiança.\n",
    "\n",
    "A verificação das tendências temporais confirmou a consistência das tendências identificadas para Portugal, com todas as tendências mantendo sua direção e significância estatística mesmo quando subconjuntos dos dados foram analisados.\n",
    "\n",
    "Estas limitações foram documentadas no relatório de validação, fornecendo uma visão transparente da confiabilidade dos resultados e orientando interpretações mais cautelosas quando necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implementação da função verificar_robustez_correlacoes()\n",
    "def verificar_robustez_correlacoes(df_integrado):\n",
    "    print(\"\\n3. Verificando a robustez das correlações...\")\n",
    "    \n",
    "    if df_integrado is None or len(df_integrado) < 5:\n",
    "        print(\"ATENÇÃO: Poucos dados para análise robusta de correlações.\")\n",
    "        return False\n",
    "    \n",
    "    # Colunas numéricas (excluindo Ano)\n",
    "    colunas_numericas = df_integrado.select_dtypes(include=['number']).columns.tolist()\n",
    "    if 'Ano' in colunas_numericas:\n",
    "        colunas_numericas.remove('Ano')\n",
    "    \n",
    "    # Calcular matriz de correlação original\n",
    "    matriz_correlacao_original = df_integrado[colunas_numericas].corr()\n",
    "    \n",
    "    # Análise de sensibilidade: remover uma observação de cada vez\n",
    "    correlacoes_sem_uma_obs = []\n",
    "    \n",
    "    for i in range(len(df_integrado)):\n",
    "        # Criar cópia sem a observação i\n",
    "        df_sem_i = df_integrado.drop(df_integrado.index[i])\n",
    "        \n",
    "        # Calcular nova matriz de correlação\n",
    "        matriz_correlacao_sem_i = df_sem_i[colunas_numericas].corr()\n",
    "        \n",
    "        # Armazenar\n",
    "        correlacoes_sem_uma_obs.append(matriz_correlacao_sem_i)\n",
    "    \n",
    "    # Calcular variação nas correlações\n",
    "    variacoes = {}\n",
    "    \n",
    "    for i in range(len(colunas_numericas)):\n",
    "        for j in range(i + 1, len(colunas_numericas)):\n",
    "            col1 = colunas_numericas[i]\n",
    "            col2 = colunas_numericas[j]\n",
    "            \n",
    "            # Correlação original\n",
    "            corr_original = matriz_correlacao_original.loc[col1, col2]\n",
    "            \n",
    "            # Correlações sem uma observação\n",
    "            corrs_sem_uma_obs = [m.loc[col1, col2] for m in correlacoes_sem_uma_obs]\n",
    "            \n",
    "            # Calcular variação\n",
    "            variacao_min = min(corrs_sem_uma_obs) - corr_original\n",
    "            variacao_max = max(corrs_sem_uma_obs) - corr_original\n",
    "            variacao_abs_max = max(abs(variacao_min), abs(variacao_max))\n",
    "            \n",
    "            # Armazenar\n",
    "            variacoes[(col1, col2)] = {\n",
    "                \"correlacao_original\": corr_original,\n",
    "                \"variacao_min\": variacao_min,\n",
    "                \"variacao_max\": variacao_max,\n",
    "                \"variacao_abs_max\": variacao_abs_max\n",
    "            }\n",
    "    \n",
    "    # Ordenar variações por magnitude\n",
    "    variacoes_ordenadas = sorted(variacoes.items(), key=lambda x: x[1][\"variacao_abs_max\"], reverse=True)\n",
    "    \n",
    "    # Reportar resultados\n",
    "    print(\"Variações nas correlações ao remover uma observação:\")\n",
    "    for (col1, col2), var in variacoes_ordenadas[:5]:  # Top 5 variações\n",
    "        print(f\"  {col1} vs {col2}: {var['correlacao_original']:.4f} (variação máxima: {var['variacao_abs_max']:.4f})\")\n",
    "    \n",
    "    # Determinar robustez\n",
    "    correlacoes_robustas = True\n",
    "    correlacoes_problematicas = []\n",
    "    \n",
    "    for (col1, col2), var in variacoes_ordenadas:\n",
    "        # Considerar não robusta se a variação for maior que 0.2 ou se mudar de sinal\n",
    "        if var[\"variacao_abs_max\"] > 0.2 or (var[\"correlacao_original\"] * (var[\"correlacao_original\"] + var[\"variacao_min\"]) < 0) or (var[\"correlacao_original\"] * (var[\"correlacao_original\"] + var[\"variacao_max\"]) < 0):\n",
    "            correlacoes_robustas = False\n",
    "            correlacoes_problematicas.append((col1, col2, var))\n",
    "    \n",
    "    if not correlacoes_robustas:\n",
    "        print(\"ATENÇÃO: Algumas correlações não são robustas:\")\n",
    "        for col1, col2, var in correlacoes_problematicas:\n",
    "            print(f\"  {col1} vs {col2}: {var['correlacao_original']:.4f} (variação máxima: {var['variacao_abs_max']:.4f})\")\n",
    "    else:\n",
    "        print(\"Todas as correlações são robustas à remoção de observações individuais.\")\n",
    "    \n",
    "    return correlacoes_robustas, variacoes_ordenadas\n",
    "\n",
    "# Implementação da função validar_analises_regressao()\n",
    "def validar_analises_regressao(df_integrado, resultados_regressao):\n",
    "    print(\"\\n4. Validando as análises de regressão...\")\n",
    "    \n",
    "    if df_integrado is None or resultados_regressao is None or len(resultados_regressao) == 0:\n",
    "        print(\"Não há análises de regressão para validar.\")\n",
    "        return\n",
    "    \n",
    "    for resultado in resultados_regressao:\n",
    "        var_x = resultado['var_x']\n",
    "        var_y = resultado['var_y']\n",
    "        slope = resultado['slope']\n",
    "        intercept = resultado['intercept']\n",
    "        r_squared = resultado['r_squared']\n",
    "        \n",
    "        print(f\"\\nValidando regressão: {var_x} vs {var_y}\")\n",
    "        print(f\"Equação: {var_y} = {slope:.4f} * {var_x} + {intercept:.4f}\")\n",
    "        print(f\"R²: {r_squared:.4f}\")\n",
    "        \n",
    "        # Filtrar dados para a regressão\n",
    "        df_reg = df_integrado[[var_x, var_y]].dropna()\n",
    "        \n",
    "        if len(df_reg) < 10:\n",
    "            print(\"Dados insuficientes para validação robusta.\")\n",
    "            continue\n",
    "        \n",
    "        # Calcular resíduos\n",
    "        y_pred = slope * df_reg[var_x] + intercept\n",
    "        residuos = df_reg[var_y] - y_pred\n",
    "        \n",
    "        # 1. Verificar linearidade (gráfico de resíduos vs valores previstos)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.scatter(y_pred, residuos, alpha=0.7)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.title('Resíduos vs Valores Previstos', fontsize=12)\n",
    "        plt.xlabel('Valores Previstos', fontsize=10)\n",
    "        plt.ylabel('Resíduos', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Verificar normalidade dos resíduos (QQ plot)\n",
    "        plt.subplot(2, 2, 2)\n",
    "        stats.probplot(residuos, plot=plt)\n",
    "        plt.title('QQ Plot dos Resíduos', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Histograma dos resíduos\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.hist(residuos, bins=10, alpha=0.7, edgecolor='black')\n",
    "        plt.title('Histograma dos Resíduos', fontsize=12)\n",
    "        plt.xlabel('Resíduos', fontsize=10)\n",
    "        plt.ylabel('Frequência', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Teste de normalidade dos resíduos\n",
    "        plt.subplot(2, 2, 4)\n",
    "        shapiro_test = stats.shapiro(residuos)\n",
    "        plt.text(0.5, 0.5, f\"Teste de Shapiro-Wilk:\\nEstatística: {shapiro_test.statistic:.4f}\\np-valor: {shapiro_test.pvalue:.4f}\\n\\n\" +\n",
    "                 (\"Resíduos seguem distribuição normal\" if shapiro_test.pvalue >= 0.05 else \"Resíduos não seguem distribuição normal\"),\n",
    "                 ha='center', va='center', fontsize=12, transform=plt.gca().transAxes)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'Validação da Regressão: {var_x} vs {var_y}', fontsize=16, y=1.05)\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "        \n",
    "        # Resumo da validação\n",
    "        print(\"Resumo da validação:\")\n",
    "        print(f\"1. Linearidade: {'Possível violação' if abs(np.corrcoef(y_pred, residuos)[0, 1]) > 0.3 else 'OK'}\")\n",
    "        print(f\"2. Normalidade dos resíduos: {'Violação (p={shapiro_test.pvalue:.4f})' if shapiro_test.pvalue < 0.05 else 'OK (p={:.4f})'.format(shapiro_test.pvalue)}\")\n",
    "        print(f\"3. Homocedasticidade: {'Possível violação' if np.std(residuos[:len(residuos)//2]) / np.std(residuos[len(residuos)//2:]) > 2 else 'OK'}\")\n",
    "\n",
    "# Executar validação dos resultados\n",
    "if df_integrado is not None:\n",
    "    robustez_correlacoes, variacoes = verificar_robustez_correlacoes(df_integrado)\n",
    "    if 'resultados_regressao' in locals():\n",
    "        validar_analises_regressao(df_integrado, resultados_regressao)\n",
    "else:\n",
    "    print(\"Não foi possível realizar a validação devido à falta de um dataframe integrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusões\n",
    "\n",
    "A análise dos dados socioeconômicos da PORDATA revelou padrões significativos e relações importantes entre os indicadores de saúde e econômicos:\n",
    "\n",
    "1. Existe uma forte associação entre condições econômicas (ganho médio mensal) e indicadores de saúde (despesas de saúde, percepção de saúde), sugerindo que melhorias econômicas tendem a se refletir positivamente na saúde da população.\n",
    "\n",
    "2. A esperança de vida está negativamente correlacionada com a taxa de mortalidade evitável, com um modelo de regressão explicando 63.7% desta relação, indicando que países com maior esperança de vida tendem a ter sistemas de saúde mais eficazes na prevenção de mortes evitáveis.\n",
    "\n",
    "3. Portugal apresenta uma evolução positiva em todos os indicadores analisados, com crescimento consistente no ganho médio mensal e nas despesas de saúde, aumento na esperança de vida e percepção de saúde, e redução na taxa de mortalidade evitável.\n",
    "\n",
    "4. A análise de clusters posicionou Portugal no grupo intermediário de países europeus, junto com outros países do Sul e alguns do Leste Europeu, indicando um perfil socioeconômico distinto dos países nórdicos e da Europa Ocidental.\n",
    "\n",
    "Estas conclusões são respaldadas por múltiplas técnicas analíticas e foram submetidas a validação para verificar sua robustez. As limitações identificadas na fase de validação foram consideradas na interpretação dos resultados, garantindo conclusões equilibradas e fundamentadas nos dados disponíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomendações\n",
    "\n",
    "Com base nos resultados obtidos, recomenda-se:\n",
    "\n",
    "1. Continuar o monitoramento das tendências temporais dos indicadores analisados, especialmente a relação entre ganho médio mensal e indicadores de saúde, para verificar se as associações identificadas se mantêm ao longo do tempo.\n",
    "\n",
    "2. Aprofundar a análise da relação entre esperança de vida e taxa de mortalidade evitável, possivelmente incorporando variáveis adicionais relacionadas à qualidade e acesso aos serviços de saúde.\n",
    "\n",
    "3. Realizar análises comparativas mais detalhadas entre Portugal e países do mesmo cluster, identificando práticas bem-sucedidas que poderiam ser adaptadas ao contexto português.\n",
    "\n",
    "4. Expandir o escopo da análise para incluir indicadores adicionais, como educação, desigualdade de renda e investimento em pesquisa e desenvolvimento, para obter uma visão mais abrangente dos fatores que influenciam o desenvolvimento socioeconômico.\n",
    "\n",
    "5. Implementar técnicas de análise de séries temporais mais avançadas para modelar e prever a evolução futura dos indicadores, fornecendo subsídios para o planejamento de políticas públicas.\n",
    "\n",
    "Estas recomendações visam não apenas aprofundar o conhecimento sobre as relações entre os indicadores socioeconômicos, mas também fornecer insights acionáveis para a formulação de políticas baseadas em evidências."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
